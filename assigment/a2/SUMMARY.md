# A2 Assignment - Summary / ä½œä¸šæ€»ç»“

## âœ… å·²å®Œæˆçš„å†…å®¹

### 1. æ ¸å¿ƒTransformerå®ç° (A2_skeleton.py)
- âœ… **A2MLP**: SwiGLUæ¶æ„çš„MLPå±‚
- âœ… **A2RMSNorm**: Root Mean Squareå½’ä¸€åŒ–å±‚
- âœ… **A2Attention**: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆåŒ…å«RoPEä½ç½®ç¼–ç ã€query/keyå½’ä¸€åŒ–ï¼‰
- âœ… **A2DecoderLayer**: å®Œæ•´çš„Transformerè§£ç å™¨å±‚ï¼ˆå«æ®‹å·®è¿æ¥ï¼‰
- âœ… **A2Transformer**: å®Œæ•´çš„è¯­è¨€æ¨¡å‹ï¼ˆembedding + layers + unembeddingï¼‰
- âœ… **A2RotaryEmbedding**: RoPEæ—‹è½¬ä½ç½®ç¼–ç å®ç°

### 2. è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬
- âœ… **train_a2.py**: å®Œæ•´çš„è®­ç»ƒè„šæœ¬
  - å¤ç”¨A1çš„tokenizerå’Œæ•°æ®å¤„ç†å·¥å…·
  - æ”¯æŒè®­ç»ƒ/éªŒè¯é›†è¯„ä¼°
  - è®¡ç®—perplexity
  - è‡ªåŠ¨ä¿å­˜æ¨¡å‹
  - Next-word predictionæ¼”ç¤º

### 3. æ–‡æœ¬ç”Ÿæˆ
- âœ… **generate_text.py**: æ–‡æœ¬ç”Ÿæˆè„šæœ¬
  - æ”¯æŒtemperatureæ§åˆ¶
  - æ”¯æŒtop-Ké‡‡æ ·
  - å¯è®¾ç½®æœ€å¤§ç”Ÿæˆé•¿åº¦
  - è‡ªåŠ¨åœæ­¢äºEOSæ ‡è®°

- âœ… **compare_generation.py**: å¯¹æ¯”è„šæœ¬
  - åŒæ—¶è¿è¡Œä½ çš„æ¨¡å‹å’Œé¢„è®­ç»ƒOLMo-2
  - ä½¿ç”¨ç›¸åŒçš„promptå’Œå‚æ•°
  - ä¾¿äºæ¯”è¾ƒç”Ÿæˆè´¨é‡

### 4. æµ‹è¯•å·¥å…·
- âœ… **sanity_check.py**: å…¨é¢çš„ç»„ä»¶æµ‹è¯•
  - æµ‹è¯•MLPå±‚
  - æµ‹è¯•RMSNorm
  - æµ‹è¯•Attentionå±‚
  - æµ‹è¯•DecoderLayer
  - æµ‹è¯•å®Œæ•´Transformer
  - æµ‹è¯•å‰å‘/åå‘ä¼ æ’­

- âœ… **test_integration.py**: A1/A2é›†æˆæµ‹è¯•
  - éªŒè¯tokenizerå…¼å®¹æ€§
  - æµ‹è¯•å®Œæ•´è®­ç»ƒæµç¨‹

### 5. éƒ¨ç½²è„šæœ¬
- âœ… **run_a2_slurm.sh**: SLURMæ‰¹å¤„ç†è„šæœ¬
  - å·²é…ç½®GPUèµ„æº
  - è‡ªåŠ¨æ¿€æ´»è¯¾ç¨‹ç¯å¢ƒ
  - é¢„è®¾åˆç†çš„è¶…å‚æ•°

- âœ… **setup_env.sh**: ç¯å¢ƒæ¿€æ´»è„šæœ¬

### 6. æ–‡æ¡£
- âœ… **README.md**: å®Œæ•´è‹±æ–‡æ–‡æ¡£
- âœ… **å¿«é€Ÿå¯åŠ¨.md**: ä¸­æ–‡å¿«é€ŸæŒ‡å—

## ğŸ¯ å®ç°è¦ç‚¹

### Architecture Details
1. **æ— åç½®é¡¹**: æ‰€æœ‰Linearå±‚ä½¿ç”¨`bias=False`ï¼ˆç¬¦åˆOLMo 2è§„èŒƒï¼‰
2. **RoPEä½ç½®ç¼–ç **: ä½¿ç”¨æ—‹è½¬ä½ç½®ç¼–ç è€Œéç»å¯¹ä½ç½®ç¼–ç 
3. **å› æœæ³¨æ„åŠ›**: ä½¿ç”¨`is_causal=True`å®ç°è‡ªå›å½’æ©ç 
4. **æ®‹å·®è¿æ¥**: åœ¨attentionå’ŒMLPåéƒ½æœ‰æ®‹å·®è¿æ¥
5. **å±‚å½’ä¸€åŒ–**: åœ¨attentionå’ŒMLPä¹‹å‰è¿›è¡Œå½’ä¸€åŒ–ï¼ˆPre-LNï¼‰

### Model Components
```
Input tokens
    â†“
Embedding
    â†“
RoPE Rotations â”€â†’ [ Transformer Layer ] Ã— N
                   â”‚                   â”‚
                   â”œâ”€ RMSNorm          â”‚
                   â”œâ”€ Multi-Head Attn  â”‚
                   â”œâ”€ Residual         â”‚
                   â”œâ”€ RMSNorm          â”‚
                   â”œâ”€ SwiGLU MLP       â”‚
                   â””â”€ Residual         â”‚
    â†“
RMSNorm
    â†“
Unembedding
    â†“
Logits [B, T, V]
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. æ¿€æ´»ç¯å¢ƒ
source /data/courses/2025_dat450_dit247/venvs/dat450_venv/bin/activate

# 2. è¿›å…¥ç›®å½•
cd /data/users/wenbota/nlp/assigment/a2

# 3. è¿è¡Œæµ‹è¯•
python sanity_check.py

# 4. æäº¤è®­ç»ƒä½œä¸š
sbatch run_a2_slurm.sh

# 5. æˆ–è€…äº¤äº’å¼è®­ç»ƒï¼ˆå°è§„æ¨¡æµ‹è¯•ï¼‰
python train_a2.py \
    --train_file /data/courses/2025_dat450_dit247/assignments/a1/train.txt \
    --val_file /data/courses/2025_dat450_dit247/assignments/a1/val.txt \
    --save_tokenizer a2_tokenizer.pkl \
    --output_dir ./a2_model_test \
    --subsample 1000 \
    --epochs 2 \
    --train_batch 8 \
    --hidden_size 128 \
    --num_layers 2 \
    --num_heads 4
```

## ğŸ“Š æµ‹è¯•ç»“æœ

å·²éªŒè¯æ‰€æœ‰ç»„ä»¶é€šè¿‡sanity checkï¼š
- âœ“ MLP Layer: è¾“å…¥è¾“å‡ºå½¢çŠ¶æ­£ç¡®
- âœ“ RMSNorm: å½’ä¸€åŒ–æ­£å¸¸å·¥ä½œ
- âœ“ Attention: å¤šå¤´æ³¨æ„åŠ›è®¡ç®—æ­£ç¡®
- âœ“ Decoder Layer: æ®‹å·®è¿æ¥æ­£å¸¸
- âœ“ Full Transformer: å®Œæ•´å‰å‘ä¼ æ’­æˆåŠŸ
- âœ“ Training Loop: å¯ä»¥æ­£å¸¸è®­ç»ƒå’Œåå‘ä¼ æ’­
- âœ“ A1 Integration: ä¸A1 tokenizeré›†æˆæˆåŠŸ

## ğŸ“ å»ºè®®çš„å®éªŒ

### 1. Next-word Prediction
```bash
python train_a2.py ... --predict_prompt "She lives in San"
```

### 2. Text Generation with Different Parameters
```bash
# Conservative generation (temperature=0.5)
python generate_text.py ... --temperature 0.5 --topk 10

# Creative generation (temperature=1.2)
python generate_text.py ... --temperature 1.2 --topk 50
```

### 3. Compare with Pre-trained Model
```bash
python compare_generation.py ... \
    --prompt "In natural language processing, a Transformer"
```

### 4. Test Different Prompts
- `"In natural language processing, a Transformer"`
- `"Is Stockholm the capital of Sweden? Answer yes or no. The answer is"`
- `"Write a Python program that reverses a list."`

## ğŸ“ å­¦ä¹ è¦ç‚¹

### å…³é”®æŠ€æœ¯
1. **Transformeræ¶æ„**: å®Œæ•´å®ç°äº†decoder-onlyæ¶æ„
2. **æ³¨æ„åŠ›æœºåˆ¶**: ç†è§£scaled dot-product attentionå’Œå¤šå¤´æ³¨æ„åŠ›
3. **ä½ç½®ç¼–ç **: RoPEçš„å·¥ä½œåŸç†
4. **å½’ä¸€åŒ–**: RMSNorm vs LayerNorm
5. **è‡ªå›å½’ç”Ÿæˆ**: Temperatureå’Œtop-Ké‡‡æ ·ç­–ç•¥

### ä½œä¸šè¦æ±‚è¦†ç›–
- âœ… Step 1: æ‰€æœ‰Transformerç»„ä»¶å·²å®ç°
- âœ… Step 2: è®­ç»ƒå’Œè¯„ä¼°åŠŸèƒ½å®Œæ•´
- âœ… Step 3: æ–‡æœ¬ç”Ÿæˆå’Œé¢„è®­ç»ƒæ¨¡å‹å¯¹æ¯”

## ğŸ“ æ–‡ä»¶ç»“æ„

```
a2/
â”œâ”€â”€ A2_skeleton.py              # ä¸»è¦å®ç°æ–‡ä»¶
â”œâ”€â”€ train_a2.py                 # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ generate_text.py            # ç”Ÿæˆè„šæœ¬
â”œâ”€â”€ compare_generation.py       # å¯¹æ¯”è„šæœ¬
â”œâ”€â”€ sanity_check.py            # æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_integration.py        # é›†æˆæµ‹è¯•
â”œâ”€â”€ run_a2_slurm.sh           # SLURMä½œä¸šè„šæœ¬
â”œâ”€â”€ setup_env.sh              # ç¯å¢ƒè®¾ç½®
â”œâ”€â”€ README.md                 # è‹±æ–‡æ–‡æ¡£
â”œâ”€â”€ å¿«é€Ÿå¯åŠ¨.md                # ä¸­æ–‡æŒ‡å—
â””â”€â”€ SUMMARY.md                # æœ¬æ–‡ä»¶
```

## ğŸ’¡ Tips

1. **å¼€å§‹æ—¶ä½¿ç”¨å°æ¨¡å‹**: `--hidden_size 128 --num_layers 2`
2. **ä½¿ç”¨subsampleå¿«é€Ÿæµ‹è¯•**: `--subsample 1000`
3. **ç›‘æ§perplexity**: åº”è¯¥é€æ¸ä¸‹é™
4. **å®éªŒä¸åŒtemperature**: è§‚å¯Ÿç”Ÿæˆè´¨é‡çš„å˜åŒ–
5. **å¯¹æ¯”é¢„è®­ç»ƒæ¨¡å‹**: ç†è§£è§„æ¨¡çš„é‡è¦æ€§

## âœ¨ ç‰¹è‰²åŠŸèƒ½

- å®Œå…¨å…¼å®¹HuggingFaceçš„`PreTrainedModel`æ¥å£
- å¯ä»¥ä½¿ç”¨`save_pretrained()`å’Œ`from_pretrained()`
- æ”¯æŒä¸A1çš„æ— ç¼é›†æˆ
- å®Œæ•´çš„é”™è¯¯æ£€æŸ¥å’Œå½¢çŠ¶éªŒè¯
- è¯¦ç»†çš„æµ‹è¯•è¦†ç›–

---

**æ‰€æœ‰ä»£ç å·²ç»å®Œæˆå¹¶æµ‹è¯•é€šè¿‡ï¼å¯ä»¥ç›´æ¥å¼€å§‹è®­ç»ƒå’Œå®éªŒã€‚** ğŸ‰
