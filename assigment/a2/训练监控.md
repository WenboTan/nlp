# é›†ç¾¤è®­ç»ƒç›‘æ§æŒ‡å—

## âœ… ä½œä¸šå·²æˆåŠŸæäº¤ï¼

### å½“å‰çŠ¶æ€
- **æµ‹è¯•ä½œä¸š**: Job ID 89685 (å·²å®Œæˆ) âœ“
  - ç»“æœï¼šè®­ç»ƒæˆåŠŸï¼PPLä»1695é™åˆ°314
  - è¾“å‡ºæ–‡ä»¶ï¼š`a2_test-89685.out`

- **å®Œæ•´è®­ç»ƒ**: Job ID 89686 (æ­£åœ¨è¿è¡Œ)
  - é¢„è®¡æ—¶é—´ï¼š6å°æ—¶
  - è¾“å‡ºæ–‡ä»¶ï¼š`a2_transformer-89686.out`

## ç›‘æ§å‘½ä»¤

### 1. æŸ¥çœ‹ä½œä¸šçŠ¶æ€
```bash
squeue -u wenbota
```
çŠ¶æ€è¯´æ˜ï¼š
- `PD` (Pending): ç­‰å¾…ä¸­
- `R` (Running): è¿è¡Œä¸­
- `CG` (Completing): å®Œæˆä¸­

### 2. å®æ—¶æŸ¥çœ‹è¾“å‡º
```bash
cd /data/users/wenbota/nlp/assigment/a2
tail -f a2_transformer-89686.out
```
æŒ‰ `Ctrl+C` é€€å‡ºæŸ¥çœ‹

### 3. æŸ¥çœ‹æœ€æ–°è¿›å±•
```bash
tail -50 a2_transformer-89686.out
```

### 4. å–æ¶ˆä½œä¸šï¼ˆå¦‚æœéœ€è¦ï¼‰
```bash
scancel 89686
```

## é¢„æœŸè¾“å‡º

è®­ç»ƒè¿‡ç¨‹ä¸­ä½ åº”è¯¥çœ‹åˆ°ï¼š

```
[Epoch 1/5] Train NLL=X.XX PPL=XXX.XX  (XX.Xs)
           Valid NLL=X.XX PPL=XXX.XX
[Epoch 2/5] Train NLL=X.XX PPL=XXX.XX  (XX.Xs)
           Valid NLL=X.XX PPL=XXX.XX
...
```

**Perplexity (PPL) åº”è¯¥é€æ¸ä¸‹é™**ï¼Œè¿™è¯´æ˜æ¨¡å‹åœ¨å­¦ä¹ ã€‚

## è®­ç»ƒå®Œæˆå

### 1. æŸ¥çœ‹æœ€ç»ˆç»“æœ
```bash
tail -100 a2_transformer-89686.out
```

åº”è¯¥èƒ½çœ‹åˆ°ï¼š
- æœ€ç»ˆçš„ Validation NLL å’Œ PPL
- Next-word prediction ç»“æœ
- æ¨¡å‹ä¿å­˜ä½ç½®

### 2. ç”Ÿæˆæ–‡æœ¬æµ‹è¯•

```bash
# æ¿€æ´»ç¯å¢ƒ
source /data/courses/2025_dat450_dit247/venvs/dat450_venv/bin/activate

# ç”Ÿæˆæ–‡æœ¬
cd /data/users/wenbota/nlp/assigment/a2
python generate_text.py \
    --model_dir ./a2_model_lr1e-4_b16_h256_l4 \
    --tokenizer_file a2_tokenizer.pkl \
    --prompt "In natural language processing, a Transformer" \
    --max_length 50 \
    --temperature 0.8 \
    --topk 50
```

### 3. å¯¹æ¯”é¢„è®­ç»ƒæ¨¡å‹

```bash
python compare_generation.py \
    --model_dir ./a2_model_lr1e-4_b16_h256_l4 \
    --tokenizer_file a2_tokenizer.pkl \
    --prompt "In natural language processing, a Transformer" \
    --max_length 50
```

## æ–‡ä»¶ä½ç½®

è®­ç»ƒåçš„æ–‡ä»¶ï¼š
```
a2/
â”œâ”€â”€ a2_tokenizer.pkl                      # Tokenizer
â”œâ”€â”€ a2_model_lr1e-4_b16_h256_l4/         # è®­ç»ƒçš„æ¨¡å‹
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ ...
â”œâ”€â”€ a2_transformer-89686.out              # è®­ç»ƒæ—¥å¿—
â””â”€â”€ ...
```

## å¦‚æœéœ€è¦è°ƒæ•´

å¦‚æœè®­ç»ƒæœ‰é—®é¢˜ï¼Œå¯ä»¥ä¿®æ”¹ `run_a2_slurm.sh` ä¸­çš„å‚æ•°ï¼š

```bash
# å‡å°æ¨¡å‹ï¼ˆæ›´å¿«è®­ç»ƒï¼‰
HIDDEN=128
LAYERS=2

# å‡å°batch sizeï¼ˆèŠ‚çœå†…å­˜ï¼‰
TRAIN_BS=8

# å‡å°‘epochsï¼ˆæ›´å¿«å®Œæˆï¼‰
EPOCHS=3

# ç„¶åé‡æ–°æäº¤
sbatch run_a2_slurm.sh
```

## å¸¸è§é—®é¢˜

**Q: å¦‚ä½•çŸ¥é“è®­ç»ƒè¿›å±•ï¼Ÿ**
A: å®šæœŸè¿è¡Œ `tail -20 a2_transformer-89686.out` æŸ¥çœ‹æœ€æ–°è¾“å‡º

**Q: è®­ç»ƒéœ€è¦å¤šä¹…ï¼Ÿ**
A: å®Œæ•´è®­ç»ƒçº¦2-6å°æ—¶ï¼Œå–å†³äºæ¨¡å‹å¤§å°å’Œæ•°æ®é‡

**Q: å¦‚ä½•éªŒè¯æ¨¡å‹è®­ç»ƒæˆåŠŸï¼Ÿ**
A: 
1. PerplexityæŒç»­ä¸‹é™
2. æœ€ç»ˆPPL < 100ï¼ˆåˆç†èŒƒå›´ï¼‰
3. èƒ½ç”Ÿæˆçœ‹èµ·æ¥åƒè‹±è¯­çš„æ–‡æœ¬

**Q: GPUå†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ**
A: ç¼–è¾‘ `run_a2_slurm.sh`ï¼Œå‡å° `TRAIN_BS` (å¦‚æ”¹ä¸º8æˆ–4)

---

**å½“å‰çŠ¶æ€**: è®­ç»ƒä½œä¸šæ­£åœ¨è¿è¡Œä¸­ ğŸš€

ä½¿ç”¨ `tail -f a2_transformer-89686.out` å®æ—¶ç›‘æ§ï¼
