# A2 作业快速启动指南

## 已完成的内容

✅ **A2_skeleton.py** - 完整的Transformer实现：
  - A2MLP: SwiGLU MLP层
  - A2RMSNorm: RMS归一化层
  - A2Attention: 多头注意力机制（含RoPE位置编码）
  - A2DecoderLayer: 完整的解码器层（含残差连接）
  - A2Transformer: 完整的Transformer语言模型

✅ **train_a2.py** - 训练脚本（复用A1的tokenizer和数据工具）

✅ **generate_text.py** - 文本生成脚本（支持temperature和top-K采样）

✅ **compare_generation.py** - 对比你的模型和预训练OLMo-2模型

✅ **sanity_check.py** - 测试所有组件

✅ **test_integration.py** - 测试与A1的集成

✅ **run_a2_slurm.sh** - SLURM批处理脚本

## 使用步骤

### 1. 激活课程Python环境

```bash
source /data/courses/2025_dat450_dit247/venvs/dat450_venv/bin/activate
```

或者使用提供的脚本：
```bash
source setup_env.sh
```

### 2. 运行sanity check（验证实现）

```bash
cd /data/users/wenbota/nlp/assigment/a2
python sanity_check.py
```

应该看到所有测试通过 ✓

### 3. 开始训练

**方式1: 交互式训练（适合调试）**

小规模快速测试：
```bash
python train_a2.py \
    --train_file /data/courses/2025_dat450_dit247/assignments/a1/train.txt \
    --val_file /data/courses/2025_dat450_dit247/assignments/a1/val.txt \
    --save_tokenizer a2_tokenizer.pkl \
    --output_dir ./a2_model_test \
    --subsample 1000 \
    --epochs 2 \
    --train_batch 8 \
    --eval_batch 8 \
    --lr 1e-4 \
    --hidden_size 128 \
    --num_layers 2 \
    --num_heads 4
```

完整训练：
```bash
python train_a2.py \
    --train_file /data/courses/2025_dat450_dit247/assignments/a1/train.txt \
    --val_file /data/courses/2025_dat450_dit247/assignments/a1/val.txt \
    --save_tokenizer a2_tokenizer.pkl \
    --output_dir ./a2_model \
    --epochs 5 \
    --train_batch 16 \
    --eval_batch 16 \
    --lr 1e-4 \
    --hidden_size 256 \
    --num_layers 4 \
    --num_heads 4
```

**方式2: 提交SLURM作业（推荐用于完整训练）**

```bash
sbatch run_a2_slurm.sh
```

查看作业状态：
```bash
squeue -u $USER
```

查看输出：
```bash
tail -f a2_train_<job_id>.out
```

### 4. 生成文本

基础生成：
```bash
python generate_text.py \
    --model_dir ./a2_model \
    --tokenizer_file a2_tokenizer.pkl \
    --prompt "In natural language processing, a Transformer" \
    --max_length 50 \
    --temperature 0.8 \
    --topk 50
```

尝试不同的参数：
- `--temperature 0.5`: 更保守的生成
- `--temperature 1.5`: 更随机的生成
- `--topk 10`: 只从前10个最可能的词中采样
- `--topk 100`: 从前100个最可能的词中采样

### 5. 对比预训练模型

```bash
python compare_generation.py \
    --model_dir ./a2_model \
    --tokenizer_file a2_tokenizer.pkl \
    --prompt "In natural language processing, a Transformer" \
    --max_length 50 \
    --temperature 0.8 \
    --topk 50
```

### 6. 测试不同的prompt

建议的测试prompt：
```bash
# Prompt 1
python generate_text.py \
    --model_dir ./a2_model \
    --tokenizer_file a2_tokenizer.pkl \
    --prompt "In natural language processing, a Transformer" \
    --max_length 50

# Prompt 2
python generate_text.py \
    --model_dir ./a2_model \
    --tokenizer_file a2_tokenizer.pkl \
    --prompt "Is Stockholm the capital of Sweden? Answer yes or no. The answer is" \
    --max_length 20

# Prompt 3
python generate_text.py \
    --model_dir ./a2_model \
    --tokenizer_file a2_tokenizer.pkl \
    --prompt "Write a Python program that reverses a list." \
    --max_length 80
```

## 模型超参数说明

- `--hidden_size`: 隐藏层维度（建议：128-512）
- `--num_layers`: Transformer层数（建议：2-6）
- `--num_heads`: 注意力头数（必须能整除hidden_size）
- `--intermediate_size`: MLP中间层大小（默认：4×hidden_size）
- `--lr`: 学习率（建议：1e-4到5e-4）
- `--train_batch`: 训练批次大小（根据GPU内存调整）
- `--epochs`: 训练轮数（建议：5-10）

## 生成参数说明

- `--temperature`: 控制随机性
  - 接近0: 几乎确定性，总是选最可能的词
  - 1.0: 标准采样
  - >1.0: 更随机，更有创意但可能不连贯

- `--topk`: Top-K采样
  - 小值(10-20): 更安全，更连贯
  - 大值(50-100): 更多样化
  - None: 从整个词汇表采样

## 文件说明

```
a2/
├── A2_skeleton.py          # Transformer实现（已完成）
├── train_a2.py             # 训练脚本
├── generate_text.py        # 文本生成
├── compare_generation.py   # 模型对比
├── sanity_check.py         # 组件测试
├── test_integration.py     # 集成测试
├── run_a2_slurm.sh        # SLURM脚本
├── setup_env.sh           # 环境激活脚本
└── README.md              # 完整文档（英文）
```

## 常见问题

**Q: 训练太慢怎么办？**
A: 使用`--subsample 5000`先在小数据集上测试，或减小模型（`--hidden_size 128 --num_layers 2`）

**Q: GPU内存不足？**
A: 减小batch size（`--train_batch 4`）或模型大小

**Q: 如何查看训练进度？**
A: 训练时会实时显示每个epoch的loss和perplexity

**Q: 模型保存在哪里？**
A: 在`--output_dir`指定的目录（默认：`./a2_model`）

## 预期结果

- **训练**: 验证集perplexity应该逐渐下降
- **生成**: 模型应该能生成看起来像英语的文本（虽然可能不太连贯）
- **对比**: 预训练OLMo-2会生成更流畅、更有意义的文本

## 提交要求

记得包含以下内容：
1. 训练输出（包括最终的validation perplexity）
2. 不同prompt和参数下的生成结果
3. 与预训练模型的对比
4. 对生成质量的反思
