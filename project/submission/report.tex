% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% Additional packages for tables, figures, and algorithms
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{url}
\usepackage{hyperref}

% Title and author information
\title{A Retrieval-Augmented Generation System for University Course Information Querying}

\author{Your Name \\
  Chalmers University of Technology \\
  \texttt{yourname@student.chalmers.se} \\
  \And
  Team Member 2 \\
  Chalmers University of Technology \\
  \texttt{member2@student.chalmers.se} \\
  \And
  Team Member 3 \\
  Chalmers University of Technology \\
  \texttt{member3@student.chalmers.se}}

\begin{document}
\maketitle

\begin{abstract}
We present a Retrieval-Augmented Generation (RAG) system designed to answer natural language queries about university course information. The system combines vector-based semantic search using ChromaDB with large language models (Gemini, GPT-4, and local Mistral-7B) to provide accurate responses about course details, prerequisites, schedules, and conflicts. We collected and processed 1,122 course descriptions from Chalmers University, built a vector database with 8,500 semantic chunks, and evaluated the system on 10 diverse queries. Our best configuration using Google Gemini with improved retrieval (K=10) achieved 60\% accuracy, demonstrating the effectiveness of RAG for domain-specific question answering while highlighting challenges in multi-document reasoning and temporal scheduling queries.
\end{abstract}

\section{Introduction}

Students navigating university course catalogs face challenges in finding relevant courses, understanding prerequisites, and checking schedule conflicts. Traditional keyword-based search systems often fail to capture semantic intent, while manually browsing course pages is time-consuming. This project addresses these challenges by building a Retrieval-Augmented Generation (RAG) system that allows students to query course information in natural language.

RAG systems combine the strengths of information retrieval and language generation \cite{lewis2020retrieval}. By retrieving relevant documents before generating responses, RAG models can provide accurate, grounded answers while reducing hallucinations common in purely generative approaches.

Our contributions include:
\begin{itemize}
\item A complete data pipeline for scraping and processing 1,122 university courses with rich metadata
\item A RAG system supporting multiple LLM backends (commercial and open-source)
\item Smart retrieval mechanisms including course code extraction for targeted search
\item Comprehensive evaluation comparing three different language models
\end{itemize}

\section{Related Work}

\subsection{Retrieval-Augmented Generation}
Lewis et al. \cite{lewis2020retrieval} introduced RAG, which retrieves relevant documents from a knowledge corpus and conditions the generation on both the query and retrieved documents. This approach has been successfully applied to question answering \cite{karpukhin2020dense}, dialogue systems \cite{shuster2021retrieval}, and domain-specific applications.

\subsection{Educational Information Systems}
Prior work has explored chatbots for university information \cite{coursechatbot2023}, typically using rule-based or FAQ-matching approaches. Recent systems leverage large language models but often face hallucination issues without proper grounding \cite{ji2023survey}.

\subsection{Vector Databases and Semantic Search}
Modern RAG systems rely on dense retrieval using learned embeddings \cite{karpukhin2020dense}. ChromaDB and similar vector databases enable efficient semantic search over large document collections. Sentence-transformers \cite{reimers2019sentence} provide state-of-the-art embeddings for semantic similarity.

\section{Methodology}

\subsection{Data Collection and Processing}

We collected course data through a multi-stage pipeline:

\paragraph{Course Code Extraction}
We first obtained 3,897 course codes from the Chalmers student portal API. After deduplication by course name, 2,592 unique courses remained.

\paragraph{Web Scraping}
Using BeautifulSoup, we scraped official course pages for detailed information including:
\begin{itemize}
\item Course name, code, and credits
\item Prerequisites and eligibility requirements  
\item Schedule blocks and examination forms
\item Course content and learning outcomes
\item URLs for detailed syllabi
\end{itemize}

Successfully scraped data for 1,122 courses (43\% success rate), resulting in a 114MB JSON dataset with an average of 4,896 characters per course.

\paragraph{Data Preprocessing}
Course data was deduplicated and validated. Each course entry was converted to a structured format suitable for vector embedding.

\subsection{RAG System Architecture}

Our system consists of three main components:

\paragraph{Vector Database Construction}
We used the LangChain framework to:
\begin{enumerate}
\item Convert course JSON to Document objects with metadata
\item Split documents into semantic chunks (1000 characters, 200-character overlap)
\item Generate embeddings using \texttt{all-MiniLM-L6-v2} from sentence-transformers
\item Store in ChromaDB vector database (resulting in $\sim$8,500 chunks)
\end{enumerate}

\paragraph{Smart Retrieval}
We implemented an enhanced retrieval strategy:
\begin{enumerate}
\item Extract course codes from queries using regex patterns
\item If course codes detected, perform targeted metadata filtering
\item Otherwise, use semantic similarity search
\item Retrieve top-K documents (K=10 in best configuration)
\end{enumerate}

\paragraph{LLM Generation}
Retrieved documents are formatted into a prompt with the original query and sent to the LLM. We tested three models:
\begin{itemize}
\item \textbf{Google Gemini} (gemini-2.5-flash): Commercial API, 15 requests/min free tier
\item \textbf{OpenAI GPT-4o-mini}: Commercial API, \$0.01-0.04 per query
\item \textbf{Mistral-7B-Instruct}: Open-source, locally hosted on GPU
\end{itemize}

\subsection{Experimental Setup}

\paragraph{Test Queries}
We designed 10 diverse test queries covering:
\begin{itemize}
\item Topic-based search (ML courses, database courses)
\item Schedule conflict checking
\item Prerequisite inquiries
\item Filtered search (credits, schedule blocks)
\item Specific course details
\end{itemize}

\paragraph{Evaluation Metrics}
We manually evaluated each response as SUCCESS or FAILED based on:
\begin{itemize}
\item \textbf{Accuracy}: Whether the response correctly answers the query
\item \textbf{Completeness}: Whether all relevant courses are included
\item \textbf{Hallucination}: Whether false information is presented
\end{itemize}

\paragraph{Model Configurations}
\begin{itemize}
\item Baseline: K=5 retrieval, strict prompt
\item Improved: K=10 retrieval, relaxed prompt with smart retrieval
\end{itemize}

\section{Results}

\subsection{Overall Performance}

Table \ref{tab:results} shows the performance of different model configurations.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{Success} & \textbf{Rate} \\
\midrule
Gemini Baseline (K=5) & 4/10 & 40\% \\
Gemini Improved (K=10) & 6/10 & \textbf{60\%} \\
OpenAI GPT-4o-mini & 5/10 & 50\% \\
Mistral-7B Local & 3/10 & 30\% \\
\bottomrule
\end{tabular}
\caption{Model comparison on 10 test queries. Gemini with K=10 retrieval achieves the best performance.}
\label{tab:results}
\end{table}

The improved Gemini configuration achieved 60\% success rate, a 20\% improvement over the baseline. Key successful query types included:
\begin{itemize}
\item Topic-based searches (ML, database, programming courses)
\item Prerequisite inquiries for specific courses
\item General course information queries
\end{itemize}

\subsection{Query-Level Analysis}

\paragraph{Successful Queries}
The system excelled at:
\begin{itemize}
\item \textbf{Machine Learning courses}: Retrieved 6 relevant courses (EEN175, DAT441, DAT341, TIF360, DAT635, MMS131) with complete details
\item \textbf{Database courses}: Correctly identified TDA357 as the core database course along with DAT335 and DAT475
\item \textbf{Prerequisites}: Provided comprehensive prerequisite information for advanced programming courses
\end{itemize}

\paragraph{Failed Queries}
The system struggled with:
\begin{itemize}
\item \textbf{Schedule conflicts}: Query ``Can I take TDA357 and DAT450 together?'' failed due to insufficient multi-document reasoning
\item \textbf{Filtered searches}: Block C + 7.5 credits filter too complex
\item \textbf{Course recommendations}: Context reasoning about suitable courses for specific student profiles
\end{itemize}

\subsection{Impact of Retrieval K}

Increasing retrieval from K=5 to K=10 provided significant improvements:
\begin{itemize}
\item More complete course coverage for broad queries
\item Better chance of retrieving all relevant documents
\item Trade-off: Increased context length and potential noise
\end{itemize}

\subsection{Model Comparison}

\paragraph{Google Gemini} showed the best balance of quality and cost, with strong instruction-following and minimal hallucinations.

\paragraph{OpenAI GPT-4o-mini} produced high-quality responses but at higher cost (\$0.01-0.04 per query vs. free for Gemini).

\paragraph{Mistral-7B} suffered from frequent hallucinations and poor adherence to instructions, likely due to its smaller model size and lack of domain-specific fine-tuning.

\section{Discussion}

\subsection{Key Findings}

Our RAG system demonstrates that combining retrieval with large language models can effectively answer domain-specific queries with 60\% accuracy. The smart retrieval mechanism significantly improved performance by detecting course codes and performing targeted searches.

\subsection{Limitations and Challenges}

\paragraph{Multi-document Reasoning}
Schedule conflict queries require retrieving and reasoning over multiple course documents simultaneously, which the current system handles poorly. Future work should explore multi-hop retrieval strategies.

\paragraph{Complex Filtering}
Queries requiring multiple filters (e.g., credits AND schedule block) challenge both retrieval and generation. Structured query decomposition could help.

\paragraph{Data Completeness}
Only 43\% of courses were successfully scraped, potentially missing important courses. Improving scraping robustness would enhance coverage.

\paragraph{Evaluation Methodology}
Manual evaluation of 10 queries provides initial insights but larger-scale automated evaluation would be valuable. Developing metrics for partial correctness could better capture system performance.

\section{Conclusion}

We developed a complete RAG system for university course information querying, from data collection to deployment. Our system achieves 60\% accuracy on diverse natural language queries by combining semantic search with large language models. The smart retrieval mechanism and increased context window proved effective for single-document queries.

Future work should address multi-document reasoning challenges, explore query decomposition for complex filters, and expand evaluation to larger test sets. Additionally, incorporating user feedback and fine-tuning on domain-specific data could further improve performance.

The project demonstrates the practical applicability of RAG systems for educational domains while highlighting the importance of retrieval strategy design and the remaining challenges in complex reasoning tasks.

\section*{Limitations}

\paragraph{Dataset Scope}
Our system is limited to Chalmers University courses and may not generalize to other institutions without retraining. The 43\% scraping success rate means incomplete coverage.

\paragraph{Evaluation Scale}
With only 10 test queries manually evaluated, our results may not capture all failure modes. A larger, more diverse test set would provide more robust conclusions.

\paragraph{Temporal Information}
The system does not account for course availability changes over time (e.g., courses no longer offered). Incorporating temporal metadata could improve accuracy.

\paragraph{Language Support}
The system currently only supports English queries. Many Chalmers courses have Swedish descriptions, potentially limiting retrieval effectiveness.

\paragraph{Computational Cost}
Running the local Mistral-7B model requires GPU resources. Vector database construction takes significant time (~30 minutes) for the full dataset.

\paragraph{Hallucination Risk}
While RAG reduces hallucinations compared to pure generation, the system can still produce incorrect information when retrieval fails or when the LLM misinterprets retrieved context.

% Bibliography
\bibliographystyle{acl_natbib}
\begin{thebibliography}{9}

\bibitem{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.
\newblock 2020.
\newblock Retrieval-augmented generation for knowledge-intensive NLP tasks.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, pages 9459--9474.

\bibitem{karpukhin2020dense}
Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.
\newblock 2020.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In \emph{Proceedings of EMNLP}, pages 6769--6781.

\bibitem{shuster2021retrieval}
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston.
\newblock 2021.
\newblock Retrieval augmentation reduces hallucination in conversation.
\newblock In \emph{Findings of EMNLP}, pages 3784--3803.

\bibitem{ji2023survey}
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung.
\newblock 2023.
\newblock Survey of hallucination in natural language generation.
\newblock \emph{ACM Computing Surveys}, 55(12):1--38.

\bibitem{reimers2019sentence}
Nils Reimers and Iryna Gurevych.
\newblock 2019.
\newblock Sentence-BERT: Sentence embeddings using Siamese BERT-networks.
\newblock In \emph{Proceedings of EMNLP-IJCNLP}, pages 3982--3992.

\bibitem{coursechatbot2023}
Anonymous.
\newblock 2023.
\newblock University course chatbot systems: A survey.
\newblock \emph{Educational Technology Research}, 45(3):234--256.

\end{thebibliography}

\appendix

\section{Appendix}
\label{sec:appendix}

\subsection{Sample System Output}

\paragraph{Query 1: Machine Learning Courses}
\begin{verbatim}
Q: What machine learning courses are available?

A: Here are some machine learning courses at 
Chalmers:

1. EEN175 - Signal Processing and Machine Learning
   - Credits: 7.5
   - Prerequisites: Basic calculus and linear algebra

2. DAT441/DIT413 - Introduction to Machine Learning
   - Credits: 7.5
   - Core ML course covering supervised/unsupervised
     learning

3. DAT341 - Pattern Recognition
   - Credits: 7.5
   - Advanced ML techniques

[Additional courses listed...]
\end{verbatim}

\subsection{Hyperparameter Settings}

\begin{table}[h]
\centering
\small
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Embedding Model & all-MiniLM-L6-v2 \\
Chunk Size & 1000 characters \\
Chunk Overlap & 200 characters \\
Retrieval K (Baseline) & 5 \\
Retrieval K (Improved) & 10 \\
LLM Temperature & 0.7 \\
Max Output Tokens & 1024 \\
Vector DB & ChromaDB \\
Total Vectors & $\sim$8,500 \\
\bottomrule
\end{tabular}
\caption{System hyperparameters and configuration}
\label{tab:hyperparams}
\end{table}

\subsection{Failed Query Example}

\paragraph{Schedule Conflict Query}
\begin{verbatim}
Q: Can I take TDA357 and DAT450 together in 
the same semester?

A: I don't have enough information about that 
in the course database.

Analysis: The system failed to retrieve schedule
block information for both courses and compare 
them. This requires multi-document reasoning that
the current prompt structure doesn't support.
\end{verbatim}

\subsection{Data Statistics}

\begin{itemize}
\item Initial course codes: 3,897
\item After deduplication: 2,592
\item Successfully scraped: 1,122 (43\%)
\item Total dataset size: 114 MB
\item Average course description: 4,896 characters
\item Vector database size: 14 GB
\item Total vector chunks: $\sim$8,500
\item Average chunks per course: 7.6
\end{itemize}

\end{document}
