# ğŸš€ Chalmers Course RAG ç³»ç»Ÿ - æœ¬åœ°æ¨¡å‹ç‰ˆ

ä½¿ç”¨å¼€æºæ¨¡å‹çš„è¯¾ç¨‹é—®ç­”ç³»ç»Ÿï¼Œå®Œå…¨åœ¨å­¦æ ¡ GPU é›†ç¾¤ä¸Šè¿è¡Œã€‚

---

## ğŸ“ é¡¹ç›®ç»“æ„ï¼ˆæ¸…ç†åï¼‰

```
/data/users/wenbota/nlp/project/
â”‚
â”œâ”€â”€ ğŸ”§ æ ¸å¿ƒè„šæœ¬
â”‚   â”œâ”€â”€ build_vector_db.py              # æ„å»ºå‘é‡æ•°æ®åº“
â”‚   â”œâ”€â”€ rag_query_system_local.py       # æœ¬åœ°æ¨¡å‹ RAG ç³»ç»Ÿ
â”‚   â””â”€â”€ test_rag_setup.py               # ç³»ç»ŸéªŒè¯å·¥å…·
â”‚
â”œâ”€â”€ ğŸ“Š æ•°æ®æ–‡ä»¶
â”‚   â””â”€â”€ chalmers_courses_full_scraped.json  # 1122 é—¨è¯¾ç¨‹ï¼ˆ114MBï¼‰
â”‚
â”œâ”€â”€ ğŸ”¨ å·¥å…·è„šæœ¬
â”‚   â”œâ”€â”€ syllabus_scraper.py             # è¯¾ç¨‹çˆ¬è™«
â”‚   â””â”€â”€ deduplicate_courses.py          # å»é‡å·¥å…·
â”‚
â”œâ”€â”€ ğŸš€ SLURM ä»»åŠ¡è„šæœ¬
â”‚   â”œâ”€â”€ run_build_db.sh                 # æ„å»ºæ•°æ®åº“ä»»åŠ¡
â”‚   â””â”€â”€ run_rag_local.sh                # è¿è¡Œ RAG ç³»ç»Ÿä»»åŠ¡
â”‚
â”œâ”€â”€ ğŸ“– æ–‡æ¡£
â”‚   â”œâ”€â”€ README.md                       # æœ¬æ–‡ä»¶
â”‚   â”œâ”€â”€ LOCAL_MODEL_GUIDE.md            # æœ¬åœ°æ¨¡å‹è¯¦ç»†æŒ‡å—
â”‚   â”œâ”€â”€ PROJECT_OVERVIEW.md             # é¡¹ç›®æ€»è§ˆ
â”‚   â””â”€â”€ RAG_README.md                   # RAG æŠ€æœ¯æ–‡æ¡£
â”‚
â””â”€â”€ ğŸ“¦ å½’æ¡£ï¼ˆå¯å¿½ç•¥ï¼‰
    â””â”€â”€ archive/                        # æ—§ç‰ˆæœ¬å’Œæµ‹è¯•æ–‡ä»¶
```

---

## ğŸ¯ å¿«é€Ÿå¼€å§‹ï¼ˆ3 æ­¥ï¼‰

### Step 1: å®‰è£…ä¾èµ–

```bash
cd /data/users/wenbota/nlp/project

# æ£€æŸ¥ Python å’Œ CUDA
module load Python/3.10.4
module load CUDA/11.7

# å®‰è£…ä¾èµ–
pip install --user -r requirements_local.txt
```

**é¢„è®¡æ—¶é—´**: 5-10 åˆ†é’Ÿ

---

### Step 2: æ„å»ºå‘é‡æ•°æ®åº“

```bash
# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p logs

# æäº¤æ„å»ºä»»åŠ¡
sbatch run_build_db.sh

# æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€
squeue -u $USER

# æŸ¥çœ‹æ—¥å¿—ï¼ˆç­‰ä»»åŠ¡å®Œæˆåï¼‰
tail -f logs/build_db_*.out
```

**é¢„è®¡æ—¶é—´**: 10-15 åˆ†é’Ÿ  
**ç”Ÿæˆå†…å®¹**: `chalmers_chroma_db/` ç›®å½•ï¼ˆçº¦ 500MBï¼‰

---

### Step 3: è¿è¡Œ RAG ç³»ç»Ÿ

```bash
# æäº¤ RAG ä»»åŠ¡
sbatch run_rag_local.sh

# æŸ¥çœ‹è¾“å‡º
tail -f logs/rag_*.out
```

**é¦–æ¬¡è¿è¡Œ**: ä¼šä¸‹è½½æ¨¡å‹ï¼ˆPhi-3-mini çº¦ 7GBï¼‰ï¼Œéœ€è¦ 15-20 åˆ†é’Ÿ  
**åç»­è¿è¡Œ**: ç«‹å³å¯åŠ¨

---

## ğŸ’¬ ä½¿ç”¨æ–¹å¼

### æ–¹å¼ A: äº¤äº’å¼æ¨¡å¼ï¼ˆé€šè¿‡ SLURM ä½œä¸šï¼‰

å¯åŠ¨ååœ¨æ—¥å¿—ä¸­æé—®ï¼š

```
ğŸ’¬ You: What machine learning courses are available?

ğŸ¤– Assistant: Here are the machine learning courses:
- TDA233: Algoritmer fÃ¶r maskininlÃ¤rning och slutledning (7.5 credits, Block D)
- TIF285: Bayesiansk dataanalys och maskininlÃ¤rning (7.5 credits, Block C)
...
```

### æ–¹å¼ B: äº¤äº’å¼ç»ˆç«¯ï¼ˆæ¨èï¼‰

å¦‚æœéœ€è¦å®æ—¶äº¤äº’ï¼Œç”³è¯·äº¤äº’å¼ GPU èŠ‚ç‚¹ï¼š

```bash
# ç”³è¯·äº¤äº’å¼ä¼šè¯
srun -p gpu --gres=gpu:1 --mem=32G --time=2:00:00 --pty bash

# åŠ è½½æ¨¡å—
module load Python/3.10.4 CUDA/11.7

# è¿è¡Œ RAG ç³»ç»Ÿ
cd /data/users/wenbota/nlp/project
python rag_query_system_local.py

# ç›´æ¥æé—®
ğŸ’¬ You: Can I take TDA357 and DAT450 together?
```

---

## ğŸ” ç¤ºä¾‹æŸ¥è¯¢

### 1. æŸ¥æ‰¾è¯¾ç¨‹
```
ğŸ’¬ You: What courses are available about artificial intelligence?
```

### 2. æ£€æŸ¥æ—¶é—´å†²çªï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰
```
ğŸ’¬ You: Can I take TDA357 and TDA233 at the same time?
ğŸ¤– Assistant: Let me check the schedules:
- TDA357 (Databases): Block D
- TDA233 (Machine Learning): Block D
âš ï¸ TIME CONFLICT: Both courses are in Block D
```

### 3. æŸ¥è¯¢å…ˆä¿®è¯¾ç¨‹
```
ğŸ’¬ You: What are the prerequisites for database courses?
```

### 4. æŸ¥è¯¢äº¤æ¢ç”Ÿèµ„æ ¼
```
ğŸ’¬ You: Which data science courses are open for exchange students?
```

---

## ğŸ›ï¸ é…ç½®é€‰é¡¹

### æ›´æ¢æ¨¡å‹

ç¼–è¾‘ `rag_query_system_local.py` ç¬¬ 18 è¡Œï¼š

```python
# å°æ¨¡å‹ï¼ˆé»˜è®¤ï¼Œæ¨èï¼‰
LLM_MODEL = "microsoft/Phi-3-mini-4k-instruct"      # ~8GB æ˜¾å­˜

# ä¸­å‹æ¨¡å‹ï¼ˆæ›´å¥½çš„è´¨é‡ï¼‰
LLM_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"  # ~14GB æ˜¾å­˜

# å¤§å‹æ¨¡å‹ï¼ˆæœ€ä½³è´¨é‡ï¼‰
LLM_MODEL = "meta-llama/Meta-Llama-3-8B-Instruct" # ~16GB æ˜¾å­˜

# ä¸­è‹±åŒè¯­
LLM_MODEL = "Qwen/Qwen2-7B-Instruct"              # ~14GB æ˜¾å­˜
```

### è°ƒæ•´æ£€ç´¢æ•°é‡

```python
RETRIEVAL_K = 5  # æ¯æ¬¡æŸ¥è¯¢æ£€ç´¢ 5 ä¸ªæœ€ç›¸å…³çš„è¯¾ç¨‹
```

### å…³é—­é‡åŒ–ï¼ˆå¦‚æœæœ‰å¤§æ˜¾å­˜ï¼‰

```python
use_8bit = False  # åœ¨ load_local_llm() è°ƒç”¨ä¸­
```

---

## ğŸ“Š èµ„æºéœ€æ±‚

| ç»„ä»¶ | CPU | å†…å­˜ | GPU æ˜¾å­˜ | ç£ç›˜ |
|------|-----|------|----------|------|
| **æ„å»ºæ•°æ®åº“** | 4-8 æ ¸ | 16GB | å¯é€‰ | 500MB |
| **RAG è¿è¡Œ (Phi-3)** | 8 æ ¸ | 16GB | 8GB | 7GB |
| **RAG è¿è¡Œ (Mistral)** | 8 æ ¸ | 24GB | 14GB | 14GB |
| **RAG è¿è¡Œ (Llama-3)** | 8 æ ¸ | 32GB | 16GB | 16GB |

**Minerva é›†ç¾¤**: é€šå¸¸æä¾› 24GB+ æ˜¾å­˜ï¼Œå¯è¿è¡Œä»»ä½•æ¨¡å‹ã€‚

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜ 1: å‘é‡æ•°æ®åº“æœªæ‰¾åˆ°
```
âŒ FileNotFoundError: chalmers_chroma_db
```
**è§£å†³**: å…ˆè¿è¡Œ `sbatch run_build_db.sh`

### é—®é¢˜ 2: GPU å†…å­˜ä¸è¶³
```
âŒ CUDA out of memory
```
**è§£å†³**: 
1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆPhi-3-miniï¼‰
2. å¯ç”¨ 8-bit é‡åŒ–ï¼š`use_8bit=True`
3. å‡å°‘ batch size

### é—®é¢˜ 3: æ¨¡å‹ä¸‹è½½å¤±è´¥
```
âŒ Connection timeout
```
**è§£å†³**:
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. ä½¿ç”¨å›½å†…é•œåƒï¼š`export HF_ENDPOINT=https://hf-mirror.com`
3. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°

### é—®é¢˜ 4: ä¾èµ–åŒ…ç¼ºå¤±
```
âŒ ImportError: No module named 'transformers'
```
**è§£å†³**: `pip install --user -r requirements_local.txt`

---

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

åŸºäº Phi-3-mini æ¨¡å‹ï¼š

- **å‘é‡æ£€ç´¢**: < 100ms
- **LLM ç”Ÿæˆ**: 2-5 ç§’ï¼ˆå–å†³äºç­”æ¡ˆé•¿åº¦ï¼‰
- **æ€»å“åº”æ—¶é—´**: é€šå¸¸ < 6 ç§’
- **å†…å­˜å ç”¨**: ~8GB GPU æ˜¾å­˜

---

## ğŸ“š æ–‡æ¡£

- **å¿«é€Ÿå¼€å§‹**: æœ¬æ–‡ä»¶
- **è¯¦ç»†æŒ‡å—**: `LOCAL_MODEL_GUIDE.md`
- **æŠ€æœ¯æ–‡æ¡£**: `RAG_README.md`
- **é¡¹ç›®æ€»è§ˆ**: `PROJECT_OVERVIEW.md`

---

## ğŸ“ æŠ€æœ¯æ ˆ

- **å‘é‡æ•°æ®åº“**: Chroma
- **åµŒå…¥æ¨¡å‹**: sentence-transformers/all-MiniLM-L6-v2
- **LLM**: HuggingFace å¼€æºæ¨¡å‹ï¼ˆPhi-3/Mistral/Llamaï¼‰
- **æ¡†æ¶**: LangChain
- **é‡åŒ–**: bitsandbytes (8-bit)

---

## âœ… æ£€æŸ¥æ¸…å•

æ„å»ºå’Œè¿è¡Œå‰ç¡®è®¤ï¼š

- [ ] å·²å®‰è£…æ‰€æœ‰ä¾èµ– (`requirements_local.txt`)
- [ ] æ•°æ®æ–‡ä»¶å­˜åœ¨ (`chalmers_courses_full_scraped.json`)
- [ ] å·²åˆ›å»ºæ—¥å¿—ç›®å½• (`mkdir -p logs`)
- [ ] æœ‰ GPU è®¿é—®æƒé™
- [ ] ç£ç›˜ç©ºé—´å……è¶³ï¼ˆè‡³å°‘ 10GBï¼‰

---

## ğŸš€ è¿›é˜¶åŠŸèƒ½

### 1. å¾®è°ƒæ¨¡å‹

ä½¿ç”¨è¯¾ç¨‹æ•°æ®å¾®è°ƒæ¨¡å‹ä»¥æå‡ä¸“ä¸šæ€§ï¼š

```bash
python finetune_model.py  # è§ LOCAL_MODEL_GUIDE.md
```

### 2. Web ç•Œé¢

éƒ¨ç½² Gradio/Streamlit ç•Œé¢ï¼š

```bash
pip install gradio
python app.py  # éœ€è¦åˆ›å»º
```

### 3. æ‰¹é‡æŸ¥è¯¢

åˆ›å»ºè„šæœ¬æ‰¹é‡å¤„ç†é—®é¢˜åˆ—è¡¨ã€‚

---

## ğŸ“ è·å–å¸®åŠ©

- **æŸ¥çœ‹æ—¥å¿—**: `tail -f logs/*.out`
- **æ£€æŸ¥ä»»åŠ¡**: `squeue -u $USER`
- **å–æ¶ˆä»»åŠ¡**: `scancel <job_id>`
- **æŸ¥çœ‹ GPU**: `nvidia-smi`

---

**æœ€åæ›´æ–°**: 2025-12-02  
**ç‰ˆæœ¬**: v1.0 - æœ¬åœ°æ¨¡å‹ç‰ˆ  
**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª
