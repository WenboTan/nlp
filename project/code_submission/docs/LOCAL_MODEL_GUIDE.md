# ğŸš€ ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ„å»º RAG ç³»ç»Ÿï¼ˆæ— éœ€ OpenAIï¼‰

## ä¸ºä»€ä¹ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Ÿ

### OpenAI æ–¹æ¡ˆçš„é—®é¢˜
- âŒ éœ€è¦ API Key å’Œä»˜è´¹
- âŒ æ•°æ®å‘é€åˆ°å¤–éƒ¨æœåŠ¡å™¨
- âŒ å—ç½‘ç»œé™åˆ¶
- âŒ æ¯æ¬¡æŸ¥è¯¢éƒ½æœ‰æˆæœ¬

### æœ¬åœ°æ¨¡å‹çš„ä¼˜åŠ¿
- âœ… **å®Œå…¨å…è´¹**ï¼ˆä¸€æ¬¡æ€§ä¸‹è½½ï¼‰
- âœ… **æ•°æ®éšç§**ï¼ˆæœ¬åœ°è¿è¡Œï¼‰
- âœ… **æ— ç½‘ç»œä¾èµ–**
- âœ… **å¯ä»¥åœ¨å­¦æ ¡ GPU é›†ç¾¤è®­ç»ƒ**
- âœ… **å®Œå…¨å¯æ§**ï¼ˆå¯ä»¥å¾®è°ƒæ¨¡å‹ï¼‰

---

## ğŸ¯ ä¸¤ç§æœ¬åœ°æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ä½¿ç”¨é¢„è®­ç»ƒå¼€æºæ¨¡å‹ï¼ˆæ¨èï¼‰
**æ— éœ€è®­ç»ƒï¼Œç›´æ¥ä½¿ç”¨**

é€‚åˆçš„å¼€æºæ¨¡å‹ï¼š
- **Mistral-7B-Instruct** - 7B å‚æ•°ï¼Œè´¨é‡é«˜
- **Llama-3-8B-Instruct** - Meta çš„æœ€æ–°æ¨¡å‹
- **Qwen2-7B-Instruct** - é˜¿é‡Œå·´å·´çš„ä¸­è‹±åŒè¯­æ¨¡å‹
- **Phi-3-mini** - å¾®è½¯çš„ 3.8B è½»é‡æ¨¡å‹

### æ–¹æ¡ˆ 2: è®­ç»ƒ/å¾®è°ƒè‡ªå·±çš„æ¨¡å‹
**åŸºäºä½ çš„è¯¾ç¨‹æ•°æ®å¾®è°ƒ**

---

## ğŸ“¦ æ–¹æ¡ˆ 1ï¼šç›´æ¥ä½¿ç”¨å¼€æºæ¨¡å‹

### Step 1: æ£€æŸ¥ GPU å¯ç”¨æ€§

```bash
# æ£€æŸ¥ CUDA
nvidia-smi

# æ£€æŸ¥ PyTorch GPU æ”¯æŒ
python3 -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPUæ•°é‡: {torch.cuda.device_count()}')"
```

### Step 2: å®‰è£…ä¾èµ–

```bash
cd /data/users/wenbota/nlp/project

# å®‰è£…å¿…è¦çš„åŒ…
pip install torch transformers accelerate bitsandbytes
pip install langchain langchain-community langchain-chroma chromadb sentence-transformers
```

### Step 3: ä¿®æ”¹ `rag_query_system.py` ä½¿ç”¨æœ¬åœ°æ¨¡å‹

åˆ›å»ºæ–°æ–‡ä»¶ `rag_query_system_local.py`ï¼š

```python
"""
æœ¬åœ°æ¨¡å‹ç‰ˆæœ¬çš„ RAG é—®ç­”ç³»ç»Ÿ
ä½¿ç”¨ HuggingFace æ¨¡å‹ï¼Œæ— éœ€ OpenAI API
"""

import os
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import HuggingFacePipeline
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser


# ===== é…ç½® =====
DB_PATH = './chalmers_chroma_db'
EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'

# é€‰æ‹©ä½ çš„ LLM æ¨¡å‹ï¼ˆæ ¹æ® GPU æ˜¾å­˜é€‰æ‹©ï¼‰
# LLM_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"  # éœ€è¦ ~14GB æ˜¾å­˜
# LLM_MODEL = "meta-llama/Llama-3-8B-Instruct"      # éœ€è¦ ~16GB æ˜¾å­˜
LLM_MODEL = "microsoft/Phi-3-mini-4k-instruct"      # åªéœ€ ~8GB æ˜¾å­˜ï¼ˆæ¨èï¼‰
# LLM_MODEL = "Qwen/Qwen2-7B-Instruct"              # ä¸­è‹±åŒè¯­

RETRIEVAL_K = 5
MAX_NEW_TOKENS = 512


def load_local_llm(model_name: str, device: str = 'cuda'):
    """
    åŠ è½½æœ¬åœ° LLM æ¨¡å‹
    
    Args:
        model_name: HuggingFace æ¨¡å‹åç§°
        device: 'cuda' æˆ– 'cpu'
    
    Returns:
        HuggingFacePipeline å®ä¾‹
    """
    print(f"Loading local LLM: {model_name}")
    print(f"Device: {device}")
    print("This may take a few minutes on first run (downloading model)...")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # åŠ è½½æ¨¡å‹
    # ä½¿ç”¨ 8-bit é‡åŒ–ä»¥å‡å°‘æ˜¾å­˜å ç”¨
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map='auto',              # è‡ªåŠ¨åˆ†é…åˆ° GPU
        torch_dtype=torch.float16,      # ä½¿ç”¨ FP16 å‡å°‘æ˜¾å­˜
        load_in_8bit=True,              # 8-bit é‡åŒ–ï¼ˆå¯é€‰ï¼Œè¿›ä¸€æ­¥å‡å°‘æ˜¾å­˜ï¼‰
        trust_remote_code=True          # æŸäº›æ¨¡å‹éœ€è¦
    )
    
    print(f"âœ“ Model loaded on {device}")
    print(f"  - Parameters: ~{sum(p.numel() for p in model.parameters()) / 1e9:.1f}B")
    print(f"  - Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
    
    # åˆ›å»º text-generation pipeline
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=MAX_NEW_TOKENS,
        temperature=0.3,                # ä½æ¸©åº¦ = æ›´ç¡®å®šçš„å›ç­”
        top_p=0.9,
        repetition_penalty=1.1
    )
    
    # åŒ…è£…ä¸º LangChain LLM
    llm = HuggingFacePipeline(pipeline=pipe)
    
    return llm


def load_vector_store(db_path: str = DB_PATH) -> Chroma:
    """åŠ è½½å‘é‡æ•°æ®åº“"""
    print(f"Loading vector database from {db_path}...")
    
    if not Path(db_path).exists():
        raise FileNotFoundError(
            f"Vector database not found at {db_path}. "
            "Please run build_vector_db.py first."
        )
    
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    vectorstore = Chroma(
        persist_directory=db_path,
        embedding_function=embeddings,
        collection_name='chalmers_courses'
    )
    
    print(f"âœ“ Vector store loaded")
    return vectorstore


def create_rag_prompt() -> ChatPromptTemplate:
    """åˆ›å»º RAG Prompt"""
    system_message = """You are a helpful Chalmers University Course Assistant. 
Answer questions about courses using the provided context.

IMPORTANT:
1. Use ONLY the retrieved context below
2. For schedule conflicts: Check "Block" field
   - Same Block = TIME CONFLICT
   - Different Block = NO CONFLICT
3. Include course codes and URLs in answers
4. If unsure, say "I don't have enough information"

Context:
{context}

Question: {question}

Answer:"""
    
    return ChatPromptTemplate.from_template(system_message)


def format_docs(docs) -> str:
    """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
    formatted = []
    for i, doc in enumerate(docs, 1):
        formatted.append(f"--- Document {i} ---")
        formatted.append(f"Course: {doc.metadata.get('course_code', 'Unknown')}")
        formatted.append(f"Block: {doc.metadata.get('block', 'Unknown')}")
        formatted.append(f"URL: {doc.metadata.get('url', 'N/A')}")
        formatted.append(f"\n{doc.page_content}\n")
    return "\n".join(formatted)


def create_rag_chain(vectorstore: Chroma, llm, k: int = RETRIEVAL_K):
    """åˆ›å»º RAG é“¾"""
    retriever = vectorstore.as_retriever(
        search_type='similarity',
        search_kwargs={'k': k}
    )
    
    prompt = create_rag_prompt()
    
    rag_chain = (
        {
            'context': retriever | format_docs,
            'question': RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain


def interactive_query_loop(rag_chain):
    """äº¤äº’å¼é—®ç­”å¾ªç¯"""
    print("\n" + "=" * 70)
    print("ğŸ“ Chalmers Course Assistant - Local Model Mode")
    print("=" * 70)
    print("\nAsk me anything about Chalmers courses!")
    print("Type 'quit', 'exit', or 'q' to stop.")
    print("=" * 70 + "\n")
    
    while True:
        try:
            question = input("\nğŸ’¬ You: ").strip()
            
            if question.lower() in ['quit', 'exit', 'q', 'bye']:
                print("\nğŸ‘‹ Goodbye!")
                break
            
            if not question:
                continue
            
            print("\nğŸ¤– Assistant: ", end='', flush=True)
            response = rag_chain.invoke(question)
            print(response)
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Interrupted. Goodbye!")
            break
        except Exception as e:
            print(f"\nâš  Error: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("Chalmers Course RAG System - Local Model")
    print("=" * 70)
    
    # æ£€æŸ¥ GPU
    if torch.cuda.is_available():
        print(f"\nâœ“ GPU detected: {torch.cuda.get_device_name(0)}")
        print(f"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        device = 'cuda'
    else:
        print("\nâš  No GPU detected, using CPU (will be slow)")
        device = 'cpu'
    
    try:
        # 1. åŠ è½½å‘é‡æ•°æ®åº“
        vectorstore = load_vector_store()
        
        # 2. åŠ è½½æœ¬åœ° LLM
        print(f"\nInitializing local LLM...")
        llm = load_local_llm(LLM_MODEL, device=device)
        print("âœ“ LLM ready")
        
        # 3. åˆ›å»º RAG é“¾
        print(f"\nBuilding RAG chain (k={RETRIEVAL_K})...")
        rag_chain = create_rag_chain(vectorstore, llm, k=RETRIEVAL_K)
        print("âœ“ RAG chain ready")
        
        # 4. å¯åŠ¨äº¤äº’å¼é—®ç­”
        interactive_query_loop(rag_chain)
    
    except FileNotFoundError as e:
        print(f"\nâŒ {e}")
        print("\nPlease run: python build_vector_db.py")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
```

### Step 4: è¿è¡Œæœ¬åœ° RAG ç³»ç»Ÿ

```bash
# é¦–å…ˆæ„å»ºå‘é‡æ•°æ®åº“ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
python build_vector_db.py

# ä½¿ç”¨æœ¬åœ°æ¨¡å‹å¯åŠ¨
python rag_query_system_local.py
```

**é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 5-15 åˆ†é’Ÿï¼Œå–å†³äºç½‘é€Ÿï¼‰**

---

## ğŸ“Š æ¨¡å‹é€‰æ‹©æŒ‡å—

| æ¨¡å‹ | å‚æ•°é‡ | æ˜¾å­˜éœ€æ±‚ | è´¨é‡ | æ¨èåœºæ™¯ |
|------|--------|----------|------|----------|
| **Phi-3-mini** | 3.8B | ~8GB | ä¸­ç­‰ | GPU æ˜¾å­˜æœ‰é™ |
| **Mistral-7B** | 7B | ~14GB | é«˜ | æœ‰è¶³å¤Ÿæ˜¾å­˜ |
| **Llama-3-8B** | 8B | ~16GB | å¾ˆé«˜ | æœ€ä½³è´¨é‡ |
| **Qwen2-7B** | 7B | ~14GB | é«˜ | ä¸­è‹±åŒè¯­ |

**åœ¨ Minerva é›†ç¾¤ä¸Š**: é€šå¸¸æœ‰ 24GB+ æ˜¾å­˜ï¼Œå¯ä»¥é€‰æ‹©ä»»ä½•æ¨¡å‹ã€‚

---

## ğŸ”§ GPU ä¼˜åŒ–æŠ€å·§

### 1. ä½¿ç”¨é‡åŒ–å‡å°‘æ˜¾å­˜

```python
# 8-bit é‡åŒ–ï¼ˆæ¨èï¼‰
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,  # æ˜¾å­˜å‡åŠ
    device_map='auto'
)

# 4-bit é‡åŒ–ï¼ˆæ›´æ¿€è¿›ï¼‰
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit=True,  # æ˜¾å­˜å‡è‡³ 1/4
    device_map='auto'
)
```

### 2. æ‰¹å¤„ç†åµŒå…¥

åœ¨ `build_vector_db.py` ä¸­ä½¿ç”¨ GPUï¼š

```python
embeddings = HuggingFaceEmbeddings(
    model_name='sentence-transformers/all-MiniLM-L6-v2',
    model_kwargs={'device': 'cuda'},  # ä½¿ç”¨ GPU
    encode_kwargs={
        'normalize_embeddings': True,
        'batch_size': 32  # æ‰¹é‡å¤„ç†
    }
)
```

### 3. åœ¨ SLURM é›†ç¾¤ä¸Šè¿è¡Œ

åˆ›å»º `run_rag.sh`:

```bash
#!/bin/bash
#SBATCH --job-name=rag_chalmers
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --mem=32G
#SBATCH --time=02:00:00

module load Python/3.10.4
module load CUDA/11.7

cd /data/users/wenbota/nlp/project

# æ„å»ºæ•°æ®åº“ï¼ˆå¦‚æœéœ€è¦ï¼‰
python build_vector_db.py

# å¯åŠ¨ RAG ç³»ç»Ÿ
python rag_query_system_local.py
```

æäº¤ä»»åŠ¡ï¼š
```bash
sbatch run_rag.sh
```

---

## ğŸ¯ æ–¹æ¡ˆ 2ï¼šå¾®è°ƒè‡ªå·±çš„æ¨¡å‹

### ä¸ºä»€ä¹ˆè¦å¾®è°ƒï¼Ÿ

ä½¿ç”¨ 1122 é—¨è¯¾ç¨‹æ•°æ®å¾®è°ƒæ¨¡å‹ï¼Œå¯ä»¥ï¼š
- âœ… æ›´å¥½åœ°ç†è§£ Chalmers è¯¾ç¨‹æœ¯è¯­
- âœ… æ›´å‡†ç¡®çš„ Block å†²çªæ£€æµ‹
- âœ… å­¦ä¹ ç‘å…¸è¯­è¯¾ç¨‹åç§°
- âœ… è®°ä½å¸¸è§è¯¾ç¨‹ä»£ç 

### å¾®è°ƒæ­¥éª¤

#### 1. å‡†å¤‡è®­ç»ƒæ•°æ®

åˆ›å»º `prepare_finetune_data.py`:

```python
import json
from datasets import Dataset

# åŠ è½½è¯¾ç¨‹æ•°æ®
with open('chalmers_courses_full_scraped.json', 'r') as f:
    courses = json.load(f)

# åˆ›å»º QA å¯¹
qa_pairs = []

for course in courses:
    code = course['course_code']
    title = course['title']
    block = course['logistics']['block']
    credits = course['logistics']['credits']
    prereq = course['constraints']['prerequisites']
    
    # ç”Ÿæˆå¤šç§é—®é¢˜
    qa_pairs.extend([
        {
            'instruction': f"What is {code}?",
            'response': f"{code} is {title}, a {credits} credit course in Block {block}."
        },
        {
            'instruction': f"Tell me about course {code}",
            'response': course['rag_text']['learning_outcomes'][:500]
        },
        {
            'instruction': f"What are the prerequisites for {code}?",
            'response': f"Prerequisites: {prereq}"
        },
        {
            'instruction': f"What block is {code} in?",
            'response': f"Block {block}"
        }
    ])

# ä¿å­˜ä¸º JSONL
dataset = Dataset.from_list(qa_pairs)
dataset.save_to_disk('chalmers_course_qa_dataset')
print(f"Created {len(qa_pairs)} QA pairs")
```

#### 2. å¾®è°ƒæ¨¡å‹

ä½¿ç”¨ HuggingFace `trl` åº“ï¼š

```bash
pip install trl peft
```

åˆ›å»º `finetune_model.py`:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import SFTTrainer
from datasets import load_from_disk

# åŠ è½½åŸºç¡€æ¨¡å‹
model_name = "microsoft/Phi-3-mini-4k-instruct"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

# åŠ è½½æ•°æ®
dataset = load_from_disk('chalmers_course_qa_dataset')

# è®­ç»ƒ
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    max_seq_length=512,
    args=TrainingArguments(
        output_dir="./chalmers_course_model",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        learning_rate=2e-5,
        fp16=True
    )
)

trainer.train()
trainer.save_model("./chalmers_course_model_final")
```

#### 3. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹

åœ¨ `rag_query_system_local.py` ä¸­ï¼š

```python
LLM_MODEL = "./chalmers_course_model_final"  # ä½¿ç”¨ä½ å¾®è°ƒçš„æ¨¡å‹
```

---

## ğŸ†š å¯¹æ¯”æ€»ç»“

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¨èåº¦ |
|------|------|------|--------|
| **OpenAI GPT** | æœ€é«˜è´¨é‡ï¼Œå³å¼€å³ç”¨ | éœ€ä»˜è´¹ï¼Œä¾èµ–ç½‘ç»œ | â­â­â­ |
| **é¢„è®­ç»ƒå¼€æºæ¨¡å‹** | å…è´¹ï¼Œè´¨é‡å¥½ | éœ€è¦ GPU | â­â­â­â­â­ |
| **å¾®è°ƒæ¨¡å‹** | æœ€é€‚é…ä½ çš„æ•°æ® | éœ€è¦è®­ç»ƒæ—¶é—´ | â­â­â­â­ |

---

## ğŸš€ æ¨èè·¯çº¿

### å¯¹äºä½ çš„æƒ…å†µï¼ˆå­¦æ ¡é›†ç¾¤ + GPUï¼‰ï¼š

1. **ç¬¬ä¸€æ­¥**: ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPhi-3-miniï¼‰
   - å¿«é€ŸéªŒè¯ RAG ç³»ç»Ÿå¯è¡Œæ€§
   - ä¸éœ€è¦è®­ç»ƒ

2. **ç¬¬äºŒæ­¥**ï¼ˆå¯é€‰ï¼‰: å¾®è°ƒæ¨¡å‹
   - ä½¿ç”¨è¯¾ç¨‹æ•°æ®å¾®è°ƒ
   - æå‡ä¸“ä¸šé¢†åŸŸè¡¨ç°

3. **ç¬¬ä¸‰æ­¥**ï¼ˆå¯é€‰ï¼‰: éƒ¨ç½²åˆ° Web
   - ä½¿ç”¨ Gradio/Streamlit
   - è®©å…¶ä»–äººä¹Ÿèƒ½ä½¿ç”¨

---

## âœ… å®Œæ•´å·¥ä½œæµç¨‹

```bash
# 1. æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆCPU å³å¯ï¼‰
python build_vector_db.py

# 2. åˆ›å»ºæœ¬åœ°æ¨¡å‹ç‰ˆæœ¬ï¼ˆä¸Šé¢æä¾›çš„ä»£ç ï¼‰
# ä¿å­˜ä¸º rag_query_system_local.py

# 3. åœ¨ GPU èŠ‚ç‚¹ä¸Šè¿è¡Œ
srun -p gpu --gres=gpu:1 --mem=32G python rag_query_system_local.py

# 4. å¼€å§‹æé—®ï¼
```

---

éœ€è¦æˆ‘å¸®ä½ åˆ›å»ºå®Œæ•´çš„ `rag_query_system_local.py` æ–‡ä»¶å—ï¼Ÿ
